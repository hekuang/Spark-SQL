{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [第1章 初探大数据](#1)\n",
    "* [第2章 Spark及其生态圈概述](#2)\n",
    "* [第3章 实战环境搭建](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(3000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 3 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# [第1章 初探大数据](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "目录：/home/hadoop\n",
    "software: 存放安装的软件\n",
    "app:存放所有软件的安装目录\n",
    "data：存放课程所有的测试数据目录\n",
    "source：存放的是软件源码的目录，如spark\n",
    "\n",
    "groupadd hadoop      创建hadoop用户组  \n",
    "useradd -m -g hadoop hadoop    新建hadoop用户并增加到hadoop用户组中  \n",
    "\n",
    "passwd hadoop     hadoop用户密码，为hadoop  \n",
    "\n",
    "环境参数<img src = 'https://upload-images.jianshu.io/upload_images/6344527-77faf8b26e275559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240' width='66%'>\n",
    "\n",
    "**software 中的软件**![s**oftware 中的软件**](https://upload-images.jianshu.io/upload_images/6344527-16dc3af5ae916884.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## 10 \n",
    "### 1.hadoop 下载\n",
    "wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz  \n",
    "tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz   -C ~/app/\n",
    "\n",
    "  \n",
    "### jdk 下载\n",
    "wget http://ftp.upf.br/pub/linux/java/jdk-7u51-linux-x64.tar.gz\n",
    "### 解压\n",
    "tar -zxvf jdk-7u51-linux-x64.tar.gz -CC ~/app/\n",
    "\n",
    "验证安装\n",
    "\n",
    "配置到系统**环境变量**\n",
    "\n",
    "vi  ~/.bash_profile  \n",
    "source ~/.bash_profile\n",
    "```\n",
    "export JAVA_HOME=/home/hadoop/app/jdk1.7.0_51\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0\n",
    "export PATH=$HADOOP_HOME/bin:$PATH\n",
    "\n",
    "export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0  \n",
    "export PATH=$HIVE_HOME/bin:$PATH  \n",
    "```\n",
    "### 3.机器参数设置\n",
    "    hostname: hekuang\n",
    "    修改：/etc/sysconfig/network\n",
    "ubuntu: /etc/network/interfaces.\n",
    "\n",
    "设置ip和hostname的映射关系：/etc/hosts\n",
    "\n",
    "\n",
    "### ssh免密码登录 \n",
    "\n",
    "ssh-keygen  -t rsa　　\n",
    "cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys  \n",
    "再追加到各个主机实现，免密码登录\n",
    "\n",
    "### hadoop配置文件修改\n",
    "* bin : 客户端操作的文件\n",
    "* sbin ： 启动集群  \n",
    "\n",
    "~/app/hadoop-2.6.0-cdh5.7.0$ cd etc/hadoop\n",
    "\n",
    "* 修改 hadoop-env.sh\n",
    "\n",
    "> `export JAVA_HOME=/home/hadoop/app/jdk1.7.0_51`\n",
    "\n",
    "*  修改 etc/hadoop/core-site.xml\n",
    "\n",
    "```\n",
    " <configuration>  \n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://207.148.26.255:8020</value>\n",
    "    </property>\n",
    "\n",
    "<property>\n",
    "        <name>hadoop.tmp.dir</name>\n",
    "        <value>/home/hadoop/app/tmp</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "* 修改 hdfs-site.xml\n",
    "```\n",
    "<property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "```\n",
    "* vi slaves\n",
    "改为本机ip\n",
    "\n",
    "* 仅一次格式化 bin/hdfs namenode -format\n",
    "* 启动 sbin/start-dfs.sh\n",
    "\n",
    "## YARN 产生背景\n",
    "* MapReduce 1.x 存在问题：单点故障，节点压力大不易扩展  \n",
    "* JobTracker : 负责资源管理和作业调度\n",
    "* TaskTracker : \n",
    "\n",
    " YARN : 不同计算框架可以共享同一个HDFS集群上的数据，享受整体的资源调度   \n",
    " XXX on YARN   Yet Another Rsesource Negotiator\n",
    "\n",
    "## YARN 架构\n",
    " YARN 架构 \n",
    "[http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)\n",
    "\n",
    "* ResouceManager ：RM   \n",
    "   整个集群同一时间提供服务的RM只有**一个**，负责集群资源的统一调度和管理  \n",
    "  监控NM， 一旦NM挂掉，该NM运行的任务需要告诉我们的AM来如何进行 处理\n",
    "  处理客户端请求：提交一个作业，杀手一个作业  \n",
    "\n",
    "* etc/hadoop/mapred-site.xml:\n",
    "cp mapred-site.xml.template mapred-site.xml\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\\\n",
    "```\n",
    "\n",
    "* etc/hadoop/yarn-site.xml:\n",
    "\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "* 启动   $ sbin/start-yarn.sh\n",
    "* 验证 ResourceManager - http://localhost:8088/\n",
    "*　提交Mr作业到YARN上运行  \n",
    "hadoop jar  /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /input/wc/hello.txt /output/wc/\n",
    "\n",
    "* 提交作业到yarn\n",
    "date 中创建txt文件  \n",
    "hadoop fs -mkdir -p /input/wc   \n",
    "hadoop fs -put  hello.txt /input/wc/\n",
    "\n",
    "## Hive\n",
    "#### 1.21 Hive产生背景\n",
    "* MapReduce 编程的不便性\n",
    "* HDFS 的文件缺少Schema  \n",
    "##### Hive 是什么  \n",
    "* Facebook开源，解决海量结构化的日志数据统计问题\n",
    "* 构建在Hadoop之上的数据仓库\n",
    "* Hive 定义类一种SQL查询语言：HQL（类似SQL但不完全相同）\n",
    "* 通常进行离线数据处理（采用MapReduce）\n",
    "* 底层支持多种不同的执行格式\n",
    "* 支持多种不同的压缩格式、存储格式以及自定义函数 压缩：GZIP、LZO、Snappy\n",
    "\n",
    "#### 1.22 \n",
    "为什么使用Hive\n",
    "* 简单，易上手\n",
    "* 为超大数据集设计的计算/存储扩展能力\n",
    "* **统一的元数据管理（可与Presto/Impala/SparkSQL等共享数据）**\n",
    "\n",
    "#### 1.23 大数据数据仓库Hive架构以及部署\n",
    "### [Hive安装配置](http://blog.51cto.com/hatech/1427748)\n",
    "#### 1-24 -Hive环境搭建\n",
    "* 下载：\n",
    "   * wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz  \n",
    "   * wget http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz\n",
    "* 解压：tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/\n",
    "\n",
    "* 配置：  \n",
    "export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0  \n",
    "export PATH=$HIVE_HOME/bin:$PATH  \n",
    "\n",
    "* 本地独立模式\n",
    "```\n",
    "# yum install mysql mysql-server          //安装mysql\n",
    "# service mysqld start\n",
    "# mysql -u root                           //添加数据库及用户\n",
    "mysql> create database hive;      \n",
    "Query OK, 1 row affected (0.00 sec)\n",
    "mysql> grant all on hive.* to 'hive'@'localhost' identified by 'hive';\n",
    "Query OK, 0 rows affected (0.00 sec)\n",
    "mysql> flush privileges;\n",
    "Query OK, 0 rows affected (0.00 sec)\n",
    "```\n",
    "\n",
    "```\n",
    "$ vim /usr/hadoop/conf/hive-site.xml \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>         //所连接的MySQL数据库实例\n",
    "  <value>jdbc:mysql://localhost:3306/sparksql?createDatabaseIfNotExist=true</value>\n",
    "  <description>JDBC connect string for a JDBC metastore</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>  //连接的MySQL数据库驱动\n",
    "  <value>com.mysql.jdbc.Driver</value>\n",
    "  <description>Driver class name for a JDBC metastore</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>   //连接的MySQL数据库用户名\n",
    "  <value>root</value>\n",
    "  <description>username to use against metastore database</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>    //连接的MySQL数据库密码\n",
    "  <value>root</value>\n",
    "  <description>password to use against metastore database</description>\n",
    "</property>\n",
    "```\n",
    "\n",
    "*   拷贝mysql驱动到$HIVE_HOME/lib/  \n",
    "cp ~/software/mysql-connector-java-5.1.46/mysql-connector-java-5.1.46.jar .\n",
    " /home/hadoop/software/mysql-connector-java-5.1.46  \n",
    "\n",
    "UNIX socket:\t\t/var/lib/mysql/mysql.sock\n",
    "\n",
    "* 启动hive\n",
    "\n",
    "/home/hadoop/app/hive-1.1.0-cdh5.7.0/conf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### [第2章 Spark及其生态圈概述](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "https://spark.apache.org/\n",
    "### 2.2. Spark 概述及特点\n",
    "分布式计算框架\n",
    "相比hadoop速度快很多\n",
    " <img src='https://spark.apache.org/images/logistic-regression.png' >\n",
    "### 2.2. Spark 产生背景\n",
    "* mapreduce的局限性\n",
    "  * 代码繁琐\n",
    "  * 只能支持map和reduce操作\n",
    "  * 执行效率低下\n",
    "  * 不适合迭代多次、交互式、流式的处理\n",
    "* 框架多样化\n",
    "  * 批处理（离线）：MapReduce，Hive，Pig\n",
    "  * 流式处理（实时）：storm，jstom\n",
    "  * 交互式计算：Impala   \n",
    "  \n",
    "> 框架多样化，导致需要多态集群，学习运维成本高\n",
    "\n",
    "### 2.4. Spark发展历史\n",
    "<img src='img/Spark发展历史.png'，width='66%'>\n",
    "### 2.6. Spark 对比 Hadoop\n",
    "### 2.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# [第3章 实战环境搭建](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Spark 源码编译\n",
    "* 下载  wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0.tgz\n",
    "* 解压  解压：tar -zxvf spark-2.1.0.tgz -C ~/app/\n",
    "* 要求： Building Spark using Maven requires Maven 3.3.9 or newer and Java 7+.\n",
    "* Maven3.3.9安装包下载 wget https://mirrors.tuna.tsinghua.edu.cn/apache//maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz\n",
    "* 解压： tar -zxvf apache-maven-3.3.9-bin.tar.gz -C ~/app/\n",
    "* maven内存设置 export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"\n",
    "* 编译命令\n",
    "  * ./build/mvn -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver  -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package\n",
    "\n",
    "  * 推荐： ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0  --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0  -Pmesos  \n",
    "  ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0  --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0  -Pmesos \n",
    "    第二个编译完成后：spark-\\$VERSINO-bin-\\$NAME \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scala 安装配置哦\n",
    "wgethttps://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz\n",
    "解压：tar -zxvf scala-2.11.8.tgz -C ~/app/\n",
    "设置Scala环境变量设置\n",
    "\n",
    "SCALA_HOME=/opt/scala-2.11.7\n",
    "PATH=$PATH:$SCALA_HOME/bin\n",
    "export SCALA_HOME PATH\n",
    "验证Scala安装\n",
    "\n",
    "scala -version\n",
    "Scala code runner version 2.11.7 -- Copyright 2002-2013, LAMP/EPFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " //将JDK目录添加到系统环境变量(~/.bash_profile)中 \n",
    "export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9 \n",
    "export PATH=$MAVEN_HOME/bin:$PATH \n",
    "//让配置文件生效 source ~/.bash_profile\n",
    "//执行mvn，查看版本\n",
    "mvn -v \n",
    "//如果安装成功后，则有如下信息的输出 \n",
    "Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T08:41:47-08:00) \n",
    "Maven home: /home/hadoop/app/apache-maven-3.3.9 Java version: 1.7.0_51, vendor: Oracle Corporation \n",
    "Java home: /home/hadoop/app/jdk1.7.0_51/jre \n",
    "Default locale: zh_CN, platform encoding: UTF-8 \n",
    "OS name: \"linux\", version: \"2.6.32-358.el6.x86_64\", arch: \"amd64\", family: \"unix\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坑1：编译报错；\n",
    "Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.1.0: Failure to find org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 in https://repo1.maven.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]\n",
    "\n",
    "坑2：虚拟机内存：2-4G\n",
    "坑3： ./dev/change-scala-version.sh 2.11\n",
    "\n",
    "https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html  \n",
    "https://spark.apache.org/docs/2.1.0/building-spark.html  \n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 报错，在pom.xml的<id>central</id>下插入\n",
    "\n",
    " \n",
    "<repository>\n",
    "    <id>cloudera</id>\n",
    "       <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n",
    "</repository\n",
    "\n",
    "<properties>\n",
    " \n",
    "    <hadoop.version>2.2.0</hadoop.version>\n",
    "    <protobuf.version>2.5.0</protobuf.version>\n",
    "    <yarn.version>${hadoop.version}</yarn.version>\n",
    "</properties>\n",
    "\n",
    "\n",
    "<profile>\n",
    "  <id>hadoop-2.6</id>\n",
    "  <properties>\n",
    "    <hadoop.version>2.6.4</hadoop.version>\n",
    "    <jets3t.version>0.9.3</jets3t.version>\n",
    "    <zookeeper.version>3.4.6</zookeeper.version>\n",
    "    <curator.version>2.6.0</curator.version>\n",
    "  </properties>\n",
    "</profile>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
