{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [第1章 初探大数据](#1)\n",
    "* [第2章 Spark及其生态圈概述](#2)\n",
    "* [第3章 实战环境搭建](#3)\n",
    "* [第4章 Spark SQL概述](#4)\n",
    "* [第5章 从Hive平滑过渡到Spark SQL](#5)\n",
    "* [第6章 DateFrame&Dataset](#6)\n",
    "* [第7章 External Data Source](#7)\n",
    "* [第8章 SparkSQL愿景](#8)\n",
    "* [第9章 慕课网日志实战](#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# [第1章 初探大数据](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "目录：/home/hadoop\n",
    "software: 存放安装的软件\n",
    "app:存放所有软件的安装目录\n",
    "data：存放课程所有的测试数据目录\n",
    "source：存放的是软件源码的目录，如spark\n",
    "\n",
    "groupadd hadoop      创建hadoop用户组  \n",
    "useradd -m -g hadoop hadoop    新建hadoop用户并增加到hadoop用户组中  \n",
    "\n",
    "passwd hadoop     hadoop用户密码，为hadoop  \n",
    "* 修改时区： http://www.cnblogs.com/kerrycode/p/4217995.html\n",
    "\n",
    "环境参数<img src = 'https://upload-images.jianshu.io/upload_images/6344527-77faf8b26e275559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240' width='66%'>\n",
    "\n",
    "**software 中的软件**![s**oftware 中的软件**](https://upload-images.jianshu.io/upload_images/6344527-16dc3af5ae916884.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## 10 \n",
    "### 1.hadoop 下载\n",
    "wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz  \n",
    "tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz   -C ~/app/\n",
    "\n",
    "  \n",
    "### jdk 下载\n",
    "wget http://ftp.upf.br/pub/linux/java/jdk-7u51-linux-x64.tar.gz\n",
    "### 解压\n",
    "tar -zxvf jdk-7u51-linux-x64.tar.gz -C ~/app/\n",
    "\n",
    "验证安装\n",
    "\n",
    "配置到系统**环境变量**\n",
    "\n",
    "vi  ~/.bash_profile  \n",
    "source ~/.bash_profile\n",
    "```\n",
    "export JAVA_HOME=/home/hadoop/app/jdk1.7.0_51\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0\n",
    "export PATH=$HADOOP_HOME/bin:$PATH\n",
    "\n",
    "export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0  \n",
    "export PATH=$HIVE_HOME/bin:$PATH  \n",
    "\n",
    "export SCALA_HOME=/home/hadoop/app/scala-2.11.8\n",
    "export PATH=$SCALA_HOME/bin:$PATH \n",
    "\n",
    "```\n",
    "### 3.机器参数设置\n",
    "    hostname: hekuang\n",
    "    修改：/etc/sysconfig/network\n",
    "ubuntu: /etc/network/interfaces.\n",
    "\n",
    "设置ip和hostname的映射关系：/etc/hosts\n",
    "\n",
    "### 静态ip\n",
    "注意：无线连接，可在路由器配置\n",
    "\n",
    "NM_CONTROLLED=yes\n",
    "BOOTPROTO=static\n",
    "IPADDR=192.168.1.106\n",
    "NETMASK=255.255.255.0\n",
    "GATEWAY=192.168.1.6\n",
    "\n",
    "\n",
    "service network restart\n",
    "\n",
    "\n",
    "\n",
    "### ssh免密码登录 \n",
    "\n",
    "ssh-keygen  -t rsa　  　  \n",
    "cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys  \n",
    "再追加到各个主机实现，免密码登录\n",
    "### 防火墙\n",
    "https://blog.csdn.net/shuaigexiaobo/article/details/78190168\n",
    "\n",
    "### hadoop配置文件修改\n",
    "* bin : 客户端操作的文件\n",
    "* sbin ： 启动集群  \n",
    "\n",
    "~/app/hadoop-2.6.0-cdh5.7.0$ cd etc/hadoop\n",
    "\n",
    "* 修改 hadoop-env.sh\n",
    "\n",
    "> `export JAVA_HOME=/home/hadoop/app/jdk1.7.0_51`\n",
    "\n",
    "*  修改 etc/hadoop/core-site.xml\n",
    "\n",
    "```\n",
    " <configuration>  \n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://IP:8020</value>\n",
    "    </property>\n",
    "\n",
    "<property>\n",
    "        <name>hadoop.tmp.dir</name>\n",
    "        <value>/home/hadoop/app/tmp</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "* 修改 hdfs-site.xml\n",
    "```\n",
    "<property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "```\n",
    "* vi slaves\n",
    "改为本机ip\n",
    "\n",
    "* 仅一次格式化 bin/hdfs namenode -format\n",
    "* 启动 sbin/start-dfs.sh\n",
    "http://192.168.128.131:50070/\n",
    "\n",
    "## YARN 产生背景\n",
    "* MapReduce 1.x 存在问题：单点故障，节点压力大不易扩展  \n",
    "* JobTracker : 负责资源管理和作业调度\n",
    "* TaskTracker : \n",
    "\n",
    " YARN : 不同计算框架可以共享同一个HDFS集群上的数据，享受整体的资源调度   \n",
    " XXX on YARN   Yet Another Rsesource Negotiator\n",
    "\n",
    "## YARN 架构\n",
    " YARN 架构 \n",
    "[http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)\n",
    "\n",
    "* ResouceManager ：RM   \n",
    "   整个集群同一时间提供服务的RM只有**一个**，负责集群资源的统一调度和管理  \n",
    "  监控NM， 一旦NM挂掉，该NM运行的任务需要告诉我们的AM来如何进行 处理\n",
    "  处理客户端请求：提交一个作业，杀手一个作业  \n",
    "\n",
    "* etc/hadoop/mapred-site.xml:\n",
    "cp mapred-site.xml.template mapred-site.xml\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\\\n",
    "```\n",
    "\n",
    "* etc/hadoop/yarn-site.xml:\n",
    "\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "* 启动   $ sbin/start-yarn.sh\n",
    "* 验证 ResourceManager - http://localhost:8088/\n",
    "*　提交Mr作业到YARN上运行  \n",
    "hadoop jar  /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /input/wc/hello.txt /output/wc/\n",
    "\n",
    "* 提交作业到yarn\n",
    "date 中创建txt文件  \n",
    "hadoop fs -mkdir -p /input/wc   \n",
    "hadoop fs -put  hello.txt /input/wc/\n",
    "\n",
    "## Hive\n",
    "\n",
    "https://cwiki.apache.org/confluence/display/Hive/Home\n",
    "\n",
    "\n",
    "#### 1.21 Hive产生背景\n",
    "* MapReduce 编程的不便性\n",
    "* HDFS 的文件缺少Schema  \n",
    "##### Hive 是什么  \n",
    "* Facebook开源，解决海量结构化的日志数据统计问题\n",
    "* 构建在Hadoop之上的数据仓库\n",
    "* Hive 定义类一种SQL查询语言：HQL（类似SQL但不完全相同）\n",
    "* 通常进行离线数据处理（采用MapReduce）\n",
    "* 底层支持多种不同的执行格式\n",
    "* 支持多种不同的压缩格式、存储格式以及自定义函数 压缩：GZIP、LZO、Snappy\n",
    "\n",
    "#### 1.22 \n",
    "为什么使用Hive\n",
    "* 简单，易上手\n",
    "* 为超大数据集设计的计算/存储扩展能力\n",
    "* **统一的元数据管理（可与Presto/Impala/SparkSQL等共享数据）**\n",
    "\n",
    "#### 1.23 大数据数据仓库Hive架构以及部署\n",
    "### [Hive安装配置](http://blog.51cto.com/hatech/1427748)\n",
    "#### 1-24 -Hive环境搭建\n",
    "* 下载：\n",
    "   * wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz  \n",
    "   * wget http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz\n",
    "   解压传入 mysql-connector-java-5.1.46-bin.jar .\n",
    "   \n",
    "* 解压：tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/\n",
    "\n",
    "* 配置：  \n",
    "export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0  \n",
    "export PATH=\\$HIVE_HOME/bin:\\$PATH  \n",
    "\n",
    "* 设置hadoop home\n",
    "  * /usr/hadoop/conf/ \n",
    "  * cp hive-env.sh.template hive-env.sh  \n",
    "  * vi hive-env.sh\n",
    "  * HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0\n",
    "\n",
    "\n",
    "\n",
    "* 本地独立模式\n",
    "注意：关闭防火墙：\n",
    "\n",
    "```\n",
    "# yum install mysql mysql-server          //安装mysql\n",
    "# service mysqld start\n",
    "# mysql -u root                           //添加数据库及用户\n",
    "mysql> create database hive;      \n",
    "Query OK, 1 row affected (0.00 sec)\n",
    "mysql> grant all on hive.* to 'hive'@'localhost' identified by 'hive';\n",
    "Query OK, 0 rows affected (0.00 sec)\n",
    "mysql> flush privileges;\n",
    "Query OK, 0 rows affected (0.00 sec)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*  拷贝mysql驱动到$HIVE_HOME/lib/  \n",
    "cp ~/software/mysql-connector-java-5.1.46-bin.jar .\n",
    " /home/hadoop/software/mysql-connector-java-5.1.46  \n",
    "\n",
    "UNIX socket:\t\t/var/lib/mysql/mysql.sock\n",
    "\n",
    "* 启动hive\n",
    "\n",
    "/home/hadoop/app/hive-1.1.0-cdh5.7.0/conf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ vim /usr/hadoop/conf/hive-site.xml \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>         //所连接的MySQL数据库实例\n",
    "  <value>jdbc:mysql://localhost:3306/sparksql?createDatabaseIfNotExist=true</value>\n",
    "  <description>JDBC connect string for a JDBC metastore</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>  //连接的MySQL数据库驱动\n",
    "  <value>com.mysql.jdbc.Driver</value>\n",
    "  <description>Driver class name for a JDBC metastore</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>   //连接的MySQL数据库用户名\n",
    "  <value>root</value>\n",
    "  <description>username to use against metastore database</description>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>    //连接的MySQL数据库密码\n",
    "  <value>root</value>\n",
    "  <description>password to use against metastore database</description>\n",
    "</property>\n",
    "\n",
    "或者：\n",
    "<configuration>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>         \n",
    "    <value>jdbc:mysql://localhost:3306/hive</value>\n",
    "    </property>\n",
    "     \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>  \n",
    "    <value>com.mysql.jdbc.Driver</value>\n",
    "      <description>Driver class name for a JDBC metastore</description>\n",
    "      </property>\n",
    "       \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>   \n",
    "    <value>hive</value>\n",
    "      <description>username to use against metastore database</description>\n",
    "      </property>\n",
    "       \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>   \n",
    "    <value>hive</value>\n",
    "      <description>password to use against metastore database</description>\n",
    "      </property>\n",
    "吧\n",
    "</configuration>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.25. Hive基本使用\n",
    "1.创建示例表如下  \n",
    "2.求每个部门人数 \n",
    "`select deptno, count(1) from emp group by deptno;`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-32e5c66b27c0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-32e5c66b27c0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    create table emp\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " \n",
    "--创建emp表\n",
    "\n",
    "\n",
    "create table emp\n",
    "(empno int ,\n",
    "ename string,\n",
    "job string,\n",
    "mgr int,\n",
    "hiredate string,\n",
    "sal double,\n",
    "comm double,\n",
    "deptno int \n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n",
    "\n",
    " \n",
    "INSERT INTO emp VALUES(7369,'SMITH','CLERK',7902,'1980-12-17',800,NULL,20);\n",
    "INSERT INTO emp VALUES(7499,'ALLEN','SALESMAN',7698,'1981-02-20',1600,300,30);\n",
    "INSERT INTO emp VALUES(7521,'WARD','SALESMAN',7698,'1981-02-22',1250,500,30);\n",
    "INSERT INTO emp VALUES(7566,'JONES','MANAGER',7839,'1981-04-02',2975,NULL,20);\n",
    "INSERT INTO emp VALUES(7654,'MARTIN','SALESMAN',7698,'1981-09-28',1250,1400,30);\n",
    "INSERT INTO emp VALUES(7698,'BLAKE','MANAGER',7839,'1981-05-01',2850,NULL,30);\n",
    "INSERT INTO emp VALUES(7782,'CLARK','MANAGER',7839,'1981-06-09',2450,NULL,10);\n",
    "INSERT INTO emp VALUES(7788,'SCOTT','ANALYST',7566,'1987-06-13',3000,NULL,20);\n",
    "INSERT INTO emp VALUES(7839,'KING','PRESIDENT',NULL,'1981-11-17',5000,NULL,10);\n",
    "INSERT INTO emp VALUES(7844,'TURNER','SALESMAN',7698,'1981-09-08',1500,0,30);\n",
    "INSERT INTO emp VALUES(7876,'ADAMS','CLERK',7788,'1987-06-13',1100,NULL,20);\n",
    "INSERT INTO emp VALUES(7900,'JAMES','CLERK',7698,'1981-12-03',950,NULL,30);\n",
    "INSERT INTO emp VALUES(7902,'FORD','ANALYST',7566,'1981-12-03',3000,NULL,20);\n",
    "INSERT INTO emp VALUES(7934,'MILLER','CLERK',7782,'1983-01-23',1300,NULL,10);\n",
    "INSERT INTO emp VALUES(8888,'HIVE','PROGRAM',7839,'1988-01-23',10300,NULL);\n",
    "\n",
    "\n",
    "--创建dept表\n",
    "\n",
    "create table dept\n",
    "(\n",
    "deptno int  ,\n",
    "dname string,\n",
    "location string\n",
    ")ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n",
    "\n",
    "\n",
    "\n",
    "INSERT INTO dept VALUES(10,'ACCOUNTING','NEW YORK');\n",
    "INSERT INTO dept VALUES(20,'RESEARCH','DALLAS');\n",
    "INSERT INTO dept VALUES(30,'SALES','CHICAGO');\n",
    "INSERT INTO dept VALUES(40,'OPERATIONS','BOSTON');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# [第2章 Spark及其生态圈概述](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "https://spark.apache.org/\n",
    "### 2.2. Spark 概述及特点\n",
    "分布式计算框架\n",
    "相比hadoop速度快很多\n",
    " <img src='https://spark.apache.org/images/logistic-regression.png' >\n",
    "### 2.2. Spark 产生背景\n",
    "* mapreduce的局限性\n",
    "  * 代码繁琐\n",
    "  * 只能支持map和reduce操作\n",
    "  * 执行效率低下\n",
    "  * 不适合迭代多次、交互式、流式的处理\n",
    "* 框架多样化\n",
    "  * 批处理（离线）：MapReduce，Hive，Pig\n",
    "  * 流式处理（实时）：storm，jstom\n",
    "  * 交互式计算：Impala   \n",
    "  \n",
    "> 框架多样化，导致需要多态集群，学习运维成本高\n",
    "\n",
    "### 2.4. Spark发展历史\n",
    "<img src='img/Spark发展历史.png'，width='66%'>\n",
    "### 2.6. Spark 对比 Hadoop\n",
    "### 2.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# [第3章 实战环境搭建](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Spark 源码编译\n",
    "* 下载  wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0.tgz\n",
    "* 解压  解压：tar -zxvf spark-2.1.0.tgz -C ~/app/\n",
    "* 要求： Building Spark using Maven requires Maven 3.3.9 or newer and Java 7+.\n",
    "* Maven3.3.9安装包下载 wget https://mirrors.tuna.tsinghua.edu.cn/apache//maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz\n",
    "* 解压： tar -zxvf apache-maven-3.3.9-bin.tar.gz -C ~/app/\n",
    "* maven内存设置 export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"\n",
    "* 编译命令\n",
    "  * ./build/mvn -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver  -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package\n",
    "\n",
    "  * 推荐： \n",
    "  \n",
    "  ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0  --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0  -Pmesos \n",
    "  \n",
    "  ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0  --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0  -Pmesos \n",
    "  \n",
    "    第二个编译完成后：spark-\\$VERSINO-bin-\\$NAME \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scala 安装配置哦\n",
    "wgethttps://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz\n",
    "解压：tar -zxvf scala-2.11.8.tgz -C ~/app/\n",
    "设置Scala环境变量设置\n",
    "\n",
    "   \n",
    "验证Scala安装  \n",
    "\n",
    "scala -version  \n",
    "Scala code runner version 2.11.7 -- Copyright 2002-2013, LAMP/EPFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-63b03db53f5c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-63b03db53f5c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    (/将JDK目录添加到系统环境变量(~/.bash_profile)中)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " //将JDK目录添加到系统环境变量(~/.bash_profile)中 \n",
    "export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9 \n",
    "export PATH=$MAVEN_HOME/bin:$PATH \n",
    "//让配置文件生效 source ~/.bash_profile\n",
    "//执行mvn，查看版本\n",
    "mvn -v \n",
    "//如果安装成功后，则有如下信息的输出 \n",
    "Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T08:41:47-08:00) \n",
    "Maven home: /home/hadoop/app/apache-maven-3.3.9 Java version: 1.7.0_51, vendor: Oracle Corporation \n",
    "Java home: /home/hadoop/app/jdk1.7.0_51/jre \n",
    "Default locale: zh_CN, platform encoding: UTF-8 \n",
    "OS name: \"linux\", version: \"2.6.32-358.el6.x86_64\", arch: \"amd64\", family: \"unix\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坑1：编译报错；\n",
    "Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.1.0: Failure to find org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 in https://repo1.maven.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]\n",
    "\n",
    "坑2：虚拟机内存：2-4G\n",
    "坑3： ./dev/change-scala-version.sh 2.11\n",
    "\n",
    "坑4：zinc https://blog.csdn.net/jiaotangX/article/details/78635133\n",
    "\n",
    "https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html  \n",
    "https://spark.apache.org/docs/2.1.0/building-spark.html  \n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571\n",
    "\n",
    "坑：zinc https://blog.csdn.net/jiaotangX/article/details/78635133  \n",
    "部分为修改项（此处是导致编译失败的根本原因）\n",
    "```\n",
    "<scalaVersion>${scala.version}</scalaVersion>\n",
    "<recompileMode>incremental</recompileMode>\n",
    "<useZincServer>false</useZincServer>\n",
    "\n",
    "```\n",
    "\n",
    "坑：\n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571\n",
    "http://www.ijava.com/article/hadoop-42617.html\n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571\n",
    "从失败的地方开始： \n",
    "https://blog.csdn.net/suisongtiao1799/article/details/80223068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e2d5d8f90c5f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e2d5d8f90c5f>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    <repository>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 报错，在pom.xml的<id>central</id>下插入\n",
    "\n",
    " \n",
    "<repository>\n",
    "    <id>cloudera</id>\n",
    "       <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n",
    "</repository>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<properties>\n",
    " \n",
    "    <hadoop.version>2.2.0</hadoop.version>\n",
    "    <protobuf.version>2.5.0</protobuf.version>\n",
    "    <yarn.version>${hadoop.version}</yarn.version>\n",
    "</properties>\n",
    "\n",
    "\n",
    "<profile>\n",
    "  <id>hadoop-2.6</id>\n",
    "  <properties>\n",
    "    <hadoop.version>2.6.4</hadoop.version>\n",
    "    <jets3t.version>0.9.3</jets3t.version>\n",
    "    <zookeeper.version>3.4.6</zookeeper.version>\n",
    "    <curator.version>2.6.0</curator.version>\n",
    "  </properties>\n",
    "</profile>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Spark Local模式环境搭建\n",
    "\n",
    "* 解压： tar -zxvf spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz -C ~/app/\n",
    "\n",
    "* 删除cmd： rm -rf *.cmd \n",
    "\n",
    "```\n",
    "export SPARK_HOME=/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0 \n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "```\n",
    " `spark-shell --master local[2]`\n",
    "\n",
    "\n",
    "### 3.5. Spark Standalone模式环境搭建\n",
    "与HDFS/YARN很类似\n",
    "1.master + n worker\n",
    "\n",
    "\n",
    "[hadoop@master conf]$ `cp spark-env.sh.template spark-env.sh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e0a65de057de>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e0a65de057de>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    ``` spark-env.sh\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Options for the daemons used in the standalone deploy mode\n",
    "\n",
    "``` spark-env.sh\n",
    "PARK_MASTER_HOST=master   \n",
    "SPARK_WORKER_CORES=2\n",
    "SPARK_WORKER_MEMORY=2g\n",
    "SPARK_WORKER_INSTANCES=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "http://www.voidcn.com/article/p-opptahtj-pb.html\n",
    "\n",
    " `SPARK_HOME/sbin/start-all.sh`\n",
    " \n",
    "\n",
    "* 提示：JAVA_HOME is not set的错误提示。\n",
    "https://blog.csdn.net/qq_27882063/article/details/58599276\n",
    "在spark-env.sh追加  JAVA_HOME 相关参数\n",
    "* 2:sbin目录一般与服务相关\n",
    " \n",
    "如果：  \n",
    "hadoop1:master  \n",
    "hadoop2:worker  \n",
    "hadoop2:worker  \n",
    "然后配置：slaves.sh  \n",
    "hadoop2  \n",
    "hadoop3  \n",
    "===》start-all.sh\n",
    "会在master上启动master进程，在slaves文件配置的所以hostname的机器上启动worker进程\n",
    "**启动**\n",
    "`spark-shell --master spark://master:7077 `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.spark 简单使用wordcount\n",
    "wordcounts\n",
    "```scala\n",
    "val file=spark.sparkContext.textFile(\"file:///home/hadoop/data/wc.txt\")\n",
    "val wordCounts = file.flatMap(line => line.split(\" ,\")).map((word => (word,1))).reduceByKey(_ + _)\n",
    "wordCounts.collect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =4></a>\n",
    "# [第4章 Spark SQL概述](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 前世今生\n",
    "关系型数据库  RDD DML\n",
    "\n",
    "* Hive: 类似于sql的Hive QL语言，sql====>mapreduce\n",
    "* Spark：hive on spark   ＝＝＝＝＞　ｓｈａｒｋ基于spark、基于内存的列式存储、与hive兼容。缺点：hive ql解析、逻辑执行。。。。。。依赖hive\n",
    "* shark终止后： 1，hive on spark 2,Spark SQL\n",
    "### 4.2. SQL on hadoop 框架\n",
    "* Hive metastone:元数据 slq to mepreduce\n",
    "* impala : cloudera开发（cdh，cm） sql\n",
    "* presto: facebook开源（京东）sql\n",
    "\n",
    "* drill:  sql 访问：hadf,rdbms json habse mangodb s3 hive\n",
    "\n",
    "* Spark Sql: sql dataframe/dataset api\n",
    "   * matastroen \n",
    "   * 访问：hadf,rdbms json habse mangodb s3 hive ===> 数据源\n",
    "   \n",
    "<img src='img/Spark_SQL架构.png' width='77%'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 5></a>\n",
    "# [第5章 从Hive平滑过渡到Spark SQL](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SQLContext 的使用\n",
    "> Spark 1.x中Spark　SQL的入口\n",
    "\n",
    "\n",
    "```Scala\n",
    "val sc: SparkContext // An existing SparkContext.\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "```\n",
    "\n",
    "### 5.2 HiveContext\n",
    "\n",
    "### 5.4 SparkSession\n",
    "> Spark 1.x中Spark　SQL的入口\n",
    "\n",
    "```Scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._\n",
    "\n",
    "```\n",
    "\n",
    "win 路径写法:`file:///D:/Data/people.json`\n",
    "\n",
    "打jar包到客户端: `D:\\Documents\\Spark\\SparkSQL>mvn clean package -DskipTests`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching Applications with spark-submit\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "    \n",
    "# examoles\n",
    "# 提交\n",
    "./bin/spark-submit \\\n",
    " --class com.imooc.spark.SQLContextAPP \\\n",
    " --master local[2] \\\n",
    " /home/hadoop/lib/sql-1.0.jar \\\n",
    "/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 spark-shell 和spark-sql 的使用\n",
    "\n",
    "#### spark-shell\n",
    "\n",
    " `spark-shell --master local[2]`\n",
    " \n",
    " * spark 控制台:   \n",
    " `spark.sql(\"show tables\").show`  \n",
    " \n",
    "* 为了在hive中读取数据,在 /home/hadoop/app/hive-1.1.0-cdh5.7.0/conf目录下:  \n",
    " `cp hive-site.xml ~/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/conf/`\n",
    " \n",
    "* 传入 jars包  \n",
    "  `spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.46-bin.jar`\n",
    "\n",
    "之后可以查看数据库的数据\n",
    "\n",
    "* 应用:\n",
    "\n",
    "hive>\n",
    "` select * from emp e join dept d on e.deptno=d.deptno;`\n",
    "\n",
    "\n",
    "> Time taken: 43.799 seconds, Fetched: 14 row(s)\n",
    "\n",
    "\n",
    "scala>\n",
    "`spark.sql(\"select * from emp e join dept d on e.deptno=d.deptno\").show`\n",
    "> 立即出结果\n",
    "\n",
    "\n",
    "* 处理报错ERROR\n",
    "`/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/conf/hive-site.xml `\n",
    "添加:\n",
    "```\n",
    "<property>\n",
    "    <name>hive.metastore.schema.verification</name>\n",
    "    <value>false</value>\n",
    "</property>\n",
    "```\n",
    "\n",
    "#### spark-sql\n",
    "* 开启:  \n",
    "`spark-sql --master local[2] --jars ~/software/mysql-connector-java-5.1.46-bin.jar`\n",
    "\n",
    "http://master:4040\n",
    "\n",
    "spark-sql>  \n",
    "` select * from emp e join dept d on e.deptno=d.deptno;`\n",
    "\n",
    "* 创建表 查看执行计划\n",
    " \n",
    "`create table t(key string, value string);`\n",
    "\n",
    "`explain select a.key*(2+3), b.value  from t a join t b on a.key = b.key and a.key > 3;`\n",
    "\n",
    ">  Physical Plan\n",
    "\n",
    "\n",
    "\n",
    "* 查看详细计划,加上extended\n",
    "`explain  extended select a.key*(2+3), b.value  from t a join t b on a.key = b.key and a.key > 3;`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "== Parsed Logical Plan ==\n",
    "'Project [unresolvedalias(('a.key * (2 + 3)), None), 'b.value]\n",
    "+- 'Join Inner, (('a.key = 'b.key) && ('a.key > 3))\n",
    "   :- 'SubqueryAlias a\n",
    "   :  +- 'UnresolvedRelation `t`\n",
    "   +- 'SubqueryAlias b\n",
    "      +- 'UnresolvedRelation `t`\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "(CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE)): double, value: string\n",
    "Project [(cast(key#25 as double) * cast((2 + 3) as double)) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#29, value#28]\n",
    "+- Join Inner, ((key#25 = key#27) && (cast(key#25 as int) > 3))\n",
    "   :- SubqueryAlias a\n",
    "   :  +- SubqueryAlias t\n",
    "   :     +- CatalogRelation `hive`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#25, value#26]\n",
    "   +- SubqueryAlias b\n",
    "      +- SubqueryAlias t\n",
    "         +- CatalogRelation `hive`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#27, value#28]\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "Project [(cast(key#25 as double) * 5.0) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#29, value#28]\n",
    "+- Join Inner, (key#25 = key#27)\n",
    "   :- Project [key#25]\n",
    "   :  +- Filter (isnotnull(key#25) && (cast(key#25 as int) > 3))\n",
    "   :     +- CatalogRelation `hive`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#25, value#26]\n",
    "   +- Filter (isnotnull(key#27) && (cast(key#27 as int) > 3))\n",
    "      +- CatalogRelation `hive`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#27, value#28]\n",
    "\n",
    "== Physical Plan ==\n",
    "*Project [(cast(key#25 as double) * 5.0) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#29, value#28]\n",
    "+- *SortMergeJoin [key#25], [key#27], Inner\n",
    "   :- *Sort [key#25 ASC NULLS FIRST], false, 0\n",
    "   :  +- Exchange hashpartitioning(key#25, 200)\n",
    "   :     +- *Filter (isnotnull(key#25) && (cast(key#25 as int) > 3))\n",
    "   :        +- HiveTableScan [key#25], CatalogRelation `hive`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#25, value#26]\n",
    "   +- *Sort [key#27 ASC NULLS FIRST], false, 0\n",
    "      +- Exchange hashpartitioning(key#27, 200)\n",
    "         +- *Filter (isnotnull(key#27) && (cast(key#27 as int) > 3))\n",
    "            +- HiveTableScan [key#27, value#28], CatalogRelation `hive`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#27, value#28]\n",
    "Time taken: 0.405 seconds, Fetched 1 row(s)\n",
    "18/07/14 09:44:28 INFO CliDriver: Time taken: 0.405 seconds, Fetched 1 row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. thriftserver/beeline的使用\n",
    "\n",
    "* 启动thriftserver : 默认端口10000  \n",
    "> /home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/sbin\n",
    "\n",
    "`./start-thriftserver.sh --master local[2] --jars ~/software/mysql-connector-java-5.1.46-bin.jar `\n",
    "\n",
    "* 验证 `jps -m`  \n",
    "SparkSubmit --master local[2]\n",
    "\n",
    "\n",
    "* 启动beeline -u 指定地址 -n 当前机器名\n",
    "\n",
    "` beeline -u jdbc:hive2://localhost:10000 -n hadoop`\n",
    "\n",
    "注： thriftserver 和普通的spark-shell/spark-sql 区别\n",
    "* spark-shell 和spark-sql都是一个spark application\n",
    "* thriftserver, 可以启动多个客户端（beeline/cod)但是是一个spark application\n",
    " > 解决了一个数据共享的问题，多个客户端可以共享数据\n",
    " \n",
    " \n",
    "### 5.7. JDBC方式编程访问\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "package com.imooc.spark\n",
    "\n",
    "import java.sql.DriverManager\n",
    "\n",
    "/**\n",
    "  * 通过JDBC的方式访问\n",
    "  */\n",
    "object SparkSQLThriftServerApp {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    Class.forName(\"org.apache.hive.jdbc.HiveDriver\")\n",
    "\n",
    "    val conn = DriverManager.getConnection(\"jdbc:hive2://master:10000\",\"hadoop\",\"\")\n",
    "    val stmt = conn.prepareStatement(\"use hive\")\n",
    "    val rst = stmt.executeQuery()\n",
    "    val pstmt = conn.prepareStatement(\"select empno, ename, sal from emp\")\n",
    "    val rs = pstmt.executeQuery()\n",
    "    while (rs.next()){\n",
    "      println(\"empno\"+rs.getInt(\"empno\")+\n",
    "        \",ename\"+rs.getString(\"ename\")+\n",
    "        \",sal:\"+rs.getDouble(\"sal\"))\n",
    "    }\n",
    "    rs.close()\n",
    "    pstmt.close()\n",
    "    conn.close()\n",
    "  }\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"6\"></a>\n",
    "# [第6章 DateFrame&Dataset](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. DataFrame 产生背景\n",
    "\n",
    "java/scala ----> R/pandas\n",
    "\n",
    "### 6.3. DataFrame 概述\n",
    "- A Dataset is a distributed collection of data. A DataFrame is a Dataset organized into named columns.(RDD with schema)\n",
    "> 以列的形式构成的分布式数据集，按照列赋予不同的名称\n",
    "\n",
    "- It is conceptually equivalent to a **table** in a relational database or a data frame in R/Python\n",
    "- An abstraction for selecting, filtering , aggregation and plotting structured data\n",
    "- Inspired by R and Pandas single machine small data processing experiences applied to distributed big data, i.e.\n",
    "### 6.4.DataFrame和RDD的对比\n",
    "<img src=\"./img/DataFrame和RDD对比.png \", width = \"66%\">\n",
    "\n",
    "\n",
    "* RDD\n",
    "    - java/scala ==> jvm \n",
    "    - python ==> python runtime\n",
    "* DataFrame \n",
    "    - java/scala/python ==> Login Plan\n",
    "    \n",
    "### 6.5. DataFrame基本API操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` scala\n",
    "package com.imooc.spark\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "/**\n",
    "  * DataFrame API基本操作\n",
    "  */\n",
    "object DataFrameApp {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val spark  = SparkSession.builder().appName(\"DataFrameApp\").master(\"local[2]\").getOrCreate()\n",
    "    //将json文件加载成DataFrame\n",
    "    val peopleDF = spark.read.format(\"json\").load(\"file:///D:/Data/people.json\")\n",
    "    // 输打DataFrame的schema信息\n",
    "    peopleDF.printSchema()\n",
    "\n",
    "    // 输出信息，默认20条\n",
    "    // peopleDF.show()\n",
    "    // 查询某列\n",
    "    peopleDF.select(\"name\").show()\n",
    "\n",
    "    peopleDF.select(peopleDF.col(\"name\"), peopleDF.col(\"age\") +10).show()\n",
    "\n",
    "    peopleDF.filter(peopleDF.col(\"age\")>29).show()\n",
    "\n",
    "    // 根据某一列进行分组，然后再进行聚合操作： select age.count(1) from table group by age\n",
    "\n",
    "    peopleDF.groupBy(\"age\").count().show()\n",
    "    spark.stop()\n",
    "    \n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. DataFrame与RDD互操作方式\n",
    "\n",
    "Spark SQL supports two different methods for converting existing RDDs into Datasets.\n",
    "- The first method uses **reflection** to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    "\n",
    "- The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.反射\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "``` scala\n",
    "package com.imooc.spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "/**\n",
    "  * DataFrame和RDD的互操作\n",
    " */\n",
    "object DataFrameRDDApp {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val spark = SparkSession.builder().appName(\"DataFrameRDDApp\").master(\"local[2]\").getOrCreate()\n",
    "    // RDD 转换为 DataFrame\n",
    "    // 1.获取RDD\n",
    "    val rdd = spark.sparkContext.textFile(\"file:///D:/Data/infos.txt\")\n",
    "    // map 将获取的RDD每一行用“，”分割,分割后的字符串转换格式\n",
    "    import spark.implicits._  // 隐式转换\n",
    "    val infoDF = rdd.map(_.split(\",\")).map(line => Info(line(0).toInt,line(1),line(2).toInt)).toDF()\n",
    "\n",
    "    infoDF.show()\n",
    "\n",
    "    infoDF.filter(infoDF.col(\"age\") > 30).show()\n",
    "\n",
    "    // 注册成临时表，可用sql直接操作\n",
    "    infoDF.createOrReplaceTempView(\"infos\")\n",
    "\n",
    "    spark.sql(\"select * from  infos where age > 30\").show()\n",
    "\n",
    "    spark.stop()\n",
    "  }\n",
    "  // 反射借助case class Info: 2,lisi,30\n",
    "  case class Info(id:Int, name:String, age:Int)\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 编程方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"7\"></a>\n",
    "# [第7章 External Data Source](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方便从不同数据源，（json,parquet,rdbms),经过混合处理（json join parquet),再将处理结果以特定的格式（json，parquet）写回到指定的系统（HDFS。S3） 上去\n",
    "\n",
    "产生：\n",
    "Spark SQL 1.2 ==> 外部数据源API\n",
    "\n",
    "#### 产生背景：\n",
    "- Loading and saving Data is not easy\n",
    "- parse raw data: text/json/parquet\n",
    "- Conver data format transformation\n",
    "- Datasets stored in various Formats/Systems\n",
    "\n",
    "#### 目标\n",
    "- Developer： build libraties for various data sources \n",
    "- User: easy loading/saving DataFrames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"9\"></a>\n",
    "# [第9章 慕课网日志实战](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.课程目录\n",
    "- 用户行为日志概述\n",
    "- 离线数据处理架构\n",
    "- 项目需求\n",
    "- 功能实现\n",
    "- Spark on YARN（目前大多数公司的办法）\n",
    "- 性能调优（重要，特别是面试）\n",
    "\n",
    "\n",
    "### 9.2.用户行为概述\n",
    "用户行为日志：用户每次访问网站时所有的行为数据(访问、浏览、搜索、点击....)\n",
    "\n",
    "- 为什么记录用户访问行为日志\n",
    "    - 网站页面的访问量\n",
    "    - 网站的黏性\n",
    "    - 推荐\n",
    "  \n",
    "- 用户行为日志生成渠道\n",
    "    - Nginx\n",
    "    - Ajax\n",
    "    \n",
    "- 用户行为日志内容\n",
    "    - 系统属性：操作系统、浏览器\n",
    "    - 访问特征：点击的 url、从哪个url跳转过来的（referer）、页面停留时间\n",
    "    - 访问信息： session_id、访问ip等\n",
    " \n",
    "- 用户行为日志分析的意义\n",
    "    - 网站的眼睛\n",
    "    - 网站的神经\n",
    "    - 网站的大脑\n",
    "    \n",
    "\n",
    "     \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.离线数据处理架构\n",
    "<img src = \"./img/离线数据处理架构.png\" width = \"88%\">\n",
    "- 数据采集  \n",
    "Flume： web日志写入到HDFS\n",
    "\n",
    "\n",
    "- 数据清洗  \n",
    "处理脏数据  \n",
    "Spark、Hive、MapReduce（或者是其他分布式计算框架）\n",
    "清洗完后存放到HDFS（hive/Spark SQL）\n",
    "\n",
    "\n",
    "- 数据处理  \n",
    "按照需求进行相应业务的统计和分析\n",
    "Spark、Hive、MapReduce（或者是其他分布式计算框架）\n",
    "\n",
    "\n",
    "- 处理结果入库  \n",
    "结果可以存放到RDBMS、NoSQL    \n",
    "\n",
    "\n",
    "- 数据可视化  \n",
    "通过图形化方式展示：饼图，柱状图，折线图  \n",
    "ECharts、HUE、Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.项目需求\n",
    "\n",
    "- 统计主站最受欢迎的课程/手记的Top N访问次数\n",
    "\n",
    "\n",
    "- 按地市统计主站最受欢迎的Top N课程   \n",
    "    根据IP地址提取出地市信息，窗口函数在Spark SQL中的使用\n",
    "    \n",
    "    \n",
    "- 按照流量统计最受欢迎的TopN课程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
