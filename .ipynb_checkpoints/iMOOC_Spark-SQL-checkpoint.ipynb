{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 　　目　录\n",
    "* [第1章 初探大数据](#1)\n",
    "* [第2章 Spark及其生态圈概述](#2)\n",
    "* [第3章 实战环境搭建](#3)\n",
    "* [第4章 Spark SQL概述](#4)\n",
    "* [第5章 从Hive平滑过渡到Spark SQL](#5)\n",
    "* [第6章 DateFrame&Dataset](#6)\n",
    "* [第7章 External Data Source](#7)\n",
    "* [第8章 SparkSQL愿景](#8)\n",
    "* [第9章 慕课网日志实战](#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# [第1章 初探大数据](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "目录：/home/hadoop\n",
    "software: 存放安装的软件\n",
    "app:存放所有软件的安装目录\n",
    "data：存放课程所有的测试数据目录\n",
    "source：存放的是软件源码的目录，如spark\n",
    "\n",
    "groupadd hadoop      创建hadoop用户组  \n",
    "useradd -m -g hadoop hadoop    新建hadoop用户并增加到hadoop用户组中  \n",
    "\n",
    "passwd hadoop     hadoop用户密码，为hadoop  \n",
    "* 修改时区： http://www.cnblogs.com/kerrycode/p/4217995.html\n",
    "\n",
    "环境参数<img src = 'https://upload-images.jianshu.io/upload_images/6344527-77faf8b26e275559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240' width='66%'>\n",
    "\n",
    "**software 中的软件**![s**oftware 中的软件**](https://upload-images.jianshu.io/upload_images/6344527-16dc3af5ae916884.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## 10 \n",
    "### 1.hadoop 下载\n",
    "wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz  \n",
    "tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz   -C ~/app/\n",
    "\n",
    "  \n",
    "### jdk 下载\n",
    "wget http://ftp.upf.br/pub/linux/java/jdk-7u51-linux-x64.tar.gz\n",
    "### 解压\n",
    "tar -zxvf jdk-7u51-linux-x64.tar.gz -C ~/app/\n",
    "\n",
    "验证安装\n",
    "\n",
    "配置到系统**环境变量**\n",
    "\n",
    "vi  ~/.bash_profile  \n",
    "source ~/.bash_profile\n",
    "```\n",
    "export JAVA_HOME=/home/hadoop/app/jdk1.7.0_51\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0\n",
    "export PATH=$HADOOP_HOME/bin:$PATH\n",
    "\n",
    "export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0  \n",
    "export PATH=$HIVE_HOME/bin:$PATH  \n",
    "\n",
    "export SCALA_HOME=/home/hadoop/app/scala-2.11.8\n",
    "export PATH=$SCALA_HOME/bin:$PATH \n",
    "\n",
    "```\n",
    "### 3.机器参数设置\n",
    "    hostname: hekuang\n",
    "    修改：/etc/sysconfig/network\n",
    "ubuntu: /etc/network/interfaces.\n",
    "\n",
    "设置ip和hostname的映射关系：/etc/hosts\n",
    "\n",
    "### 静态ip\n",
    "注意：无线连接，可在路由器配置\n",
    "\n",
    "NM_CONTROLLED=yes\n",
    "BOOTPROTO=static\n",
    "IPADDR=192.168.1.106\n",
    "NETMASK=255.255.255.0\n",
    "GATEWAY=192.168.1.6\n",
    "\n",
    "\n",
    "service network restart\n",
    "\n",
    "\n",
    "\n",
    "### ssh免密码登录 \n",
    "\n",
    "ssh-keygen  -t rsa　  　  \n",
    "cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys  \n",
    "再追加到各个主机实现，免密码登录\n",
    "### 防火墙\n",
    "https://blog.csdn.net/shuaigexiaobo/article/details/78190168\n",
    "\n",
    "### hadoop配置文件修改\n",
    "* bin : 客户端操作的文件\n",
    "* sbin ： 启动集群  \n",
    "\n",
    "~/app/hadoop-2.6.0-cdh5.7.0$ cd etc/hadoop\n",
    "\n",
    "* 修改 hadoop-env.sh\n",
    "\n",
    "> `export JAVA_HOME=/home/hadoop/app/jdk1.7.0_51`\n",
    "\n",
    "*  修改 etc/hadoop/core-site.xml\n",
    "\n",
    "```\n",
    " <configuration>  \n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://IP:8020</value>\n",
    "    </property>\n",
    "\n",
    "<property>\n",
    "        <name>hadoop.tmp.dir</name>\n",
    "        <value>/home/hadoop/app/tmp</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "* 修改 hdfs-site.xml\n",
    "```\n",
    "<property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "```\n",
    "* vi slaves\n",
    "改为本机ip\n",
    "\n",
    "* 仅一次格式化 bin/hdfs namenode -format\n",
    "* 启动 sbin/start-dfs.sh\n",
    "http://192.168.128.131:50070/\n",
    "\n",
    "## YARN 产生背景\n",
    "* MapReduce 1.x 存在问题：单点故障，节点压力大不易扩展  \n",
    "* JobTracker : 负责资源管理和作业调度\n",
    "* TaskTracker : \n",
    "\n",
    " YARN : 不同计算框架可以共享同一个HDFS集群上的数据，享受整体的资源调度   \n",
    " XXX on YARN   Yet Another Rsesource Negotiator\n",
    "\n",
    "## YARN 架构\n",
    " YARN 架构 \n",
    "[http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)\n",
    "\n",
    "* ResouceManager ：RM   \n",
    "   整个集群同一时间提供服务的RM只有**一个**，负责集群资源的统一调度和管理  \n",
    "  监控NM， 一旦NM挂掉，该NM运行的任务需要告诉我们的AM来如何进行 处理\n",
    "  处理客户端请求：提交一个作业，杀手一个作业  \n",
    "\n",
    "* etc/hadoop/mapred-site.xml:\n",
    "cp mapred-site.xml.template mapred-site.xml\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\\\n",
    "```\n",
    "\n",
    "* etc/hadoop/yarn-site.xml:\n",
    "\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "* 启动   $ sbin/start-yarn.sh\n",
    "* 验证 ResourceManager - http://localhost:8088/\n",
    "*　提交Mr作业到YARN上运行  \n",
    "hadoop jar  /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /input/wc/hello.txt /output/wc/\n",
    "\n",
    "* 提交作业到yarn\n",
    "date 中创建txt文件  \n",
    "hadoop fs -mkdir -p /input/wc   \n",
    "hadoop fs -put  hello.txt /input/wc/\n",
    "\n",
    "## Hive\n",
    "\n",
    "https://cwiki.apache.org/confluence/display/Hive/Home\n",
    "\n",
    "\n",
    "#### 1.21 Hive产生背景\n",
    "* MapReduce 编程的不便性\n",
    "* HDFS 的文件缺少Schema  \n",
    "##### Hive 是什么  \n",
    "* Facebook开源，解决海量结构化的日志数据统计问题\n",
    "* 构建在Hadoop之上的数据仓库\n",
    "* Hive 定义类一种SQL查询语言：HQL（类似SQL但不完全相同）\n",
    "* 通常进行离线数据处理（采用MapReduce）\n",
    "* 底层支持多种不同的执行格式\n",
    "* 支持多种不同的压缩格式、存储格式以及自定义函数 压缩：GZIP、LZO、Snappy\n",
    "\n",
    "#### 1.22 \n",
    "为什么使用Hive\n",
    "* 简单，易上手\n",
    "* 为超大数据集设计的计算/存储扩展能力\n",
    "* **统一的元数据管理（可与Presto/Impala/SparkSQL等共享数据）**\n",
    "\n",
    "#### 1.23 大数据数据仓库Hive架构以及部署\n",
    "### [Hive安装配置](http://blog.51cto.com/hatech/1427748)\n",
    "#### 1-24 -Hive环境搭建\n",
    "* 下载：\n",
    "   * wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz  \n",
    "   * wget http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz\n",
    "   解压传入 mysql-connector-java-5.1.46-bin.jar .\n",
    "   \n",
    "* 解压：tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/\n",
    "\n",
    "* 配置：  \n",
    "export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0  \n",
    "export PATH=\\$HIVE_HOME/bin:\\$PATH  \n",
    "\n",
    "* 设置hadoop home\n",
    "  * /usr/hadoop/conf/ \n",
    "  * cp hive-env.sh.template hive-env.sh  \n",
    "  * vi hive-env.sh\n",
    "  * HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0\n",
    "\n",
    "\n",
    "\n",
    "* 本地独立模式\n",
    "\n",
    "```\n",
    "# yum install mysql mysql-server          //安装mysql\n",
    "# service mysqld start\n",
    "# mysql -u root                           //添加数据库及用户\n",
    "mysql> create database hive;      \n",
    "Query OK, 1 row affected (0.00 sec)\n",
    "mysql> grant all on hive.* to 'hive'@'localhost' identified by 'hive';\n",
    "Query OK, 0 rows affected (0.00 sec)\n",
    "mysql> flush privileges;\n",
    "Query OK, 0 rows affected (0.00 sec)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*  拷贝mysql驱动到$HIVE_HOME/lib/  \n",
    "cp ~/software/mysql-connector-java-5.1.46-bin.jar .\n",
    " /home/hadoop/software/mysql-connector-java-5.1.46  \n",
    "\n",
    "UNIX socket:\t\t/var/lib/mysql/mysql.sock\n",
    "\n",
    "* 启动hive\n",
    "\n",
    "/home/hadoop/app/hive-1.1.0-cdh5.7.0/conf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ vim /usr/hadoop/conf/hive-site.xml \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>         //所连接的MySQL数据库实例\n",
    "  <value>jdbc:mysql://localhost:3306/sparksql?createDatabaseIfNotExist=true</value>\n",
    "  <description>JDBC connect string for a JDBC metastore</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>  //连接的MySQL数据库驱动\n",
    "  <value>com.mysql.jdbc.Driver</value>\n",
    "  <description>Driver class name for a JDBC metastore</description>\n",
    "</property>\n",
    " \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>   //连接的MySQL数据库用户名\n",
    "  <value>root</value>\n",
    "  <description>username to use against metastore database</description>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>    //连接的MySQL数据库密码\n",
    "  <value>root</value>\n",
    "  <description>password to use against metastore database</description>\n",
    "</property>\n",
    "\n",
    "或者：\n",
    "<configuration>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>         \n",
    "    <value>jdbc:mysql://localhost:3306/hive</value>\n",
    "    </property>\n",
    "     \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>  \n",
    "    <value>com.mysql.jdbc.Driver</value>\n",
    "      <description>Driver class name for a JDBC metastore</description>\n",
    "      </property>\n",
    "       \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>   \n",
    "    <value>hive</value>\n",
    "      <description>username to use against metastore database</description>\n",
    "      </property>\n",
    "       \n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>   \n",
    "    <value>hive</value>\n",
    "      <description>password to use against metastore database</description>\n",
    "      </property>\n",
    "吧\n",
    "</configuration>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.25. Hive基本使用\n",
    "1.创建示例表如下  \n",
    "2.求每个部门人数 \n",
    "`select deptno, count(1) from emp group by deptno;`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-32e5c66b27c0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-32e5c66b27c0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    create table emp\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " \n",
    "--创建emp表\n",
    "\n",
    "\n",
    "create table emp\n",
    "(empno int ,\n",
    "ename string,\n",
    "job string,\n",
    "mgr int,\n",
    "hiredate string,\n",
    "sal double,\n",
    "comm double,\n",
    "deptno int \n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n",
    "\n",
    " \n",
    "INSERT INTO emp VALUES(7369,'SMITH','CLERK',7902,'1980-12-17',800,NULL,20);\n",
    "INSERT INTO emp VALUES(7499,'ALLEN','SALESMAN',7698,'1981-02-20',1600,300,30);\n",
    "INSERT INTO emp VALUES(7521,'WARD','SALESMAN',7698,'1981-02-22',1250,500,30);\n",
    "INSERT INTO emp VALUES(7566,'JONES','MANAGER',7839,'1981-04-02',2975,NULL,20);\n",
    "INSERT INTO emp VALUES(7654,'MARTIN','SALESMAN',7698,'1981-09-28',1250,1400,30);\n",
    "INSERT INTO emp VALUES(7698,'BLAKE','MANAGER',7839,'1981-05-01',2850,NULL,30);\n",
    "INSERT INTO emp VALUES(7782,'CLARK','MANAGER',7839,'1981-06-09',2450,NULL,10);\n",
    "INSERT INTO emp VALUES(7788,'SCOTT','ANALYST',7566,'1987-06-13',3000,NULL,20);\n",
    "INSERT INTO emp VALUES(7839,'KING','PRESIDENT',NULL,'1981-11-17',5000,NULL,10);\n",
    "INSERT INTO emp VALUES(7844,'TURNER','SALESMAN',7698,'1981-09-08',1500,0,30);\n",
    "INSERT INTO emp VALUES(7876,'ADAMS','CLERK',7788,'1987-06-13',1100,NULL,20);\n",
    "INSERT INTO emp VALUES(7900,'JAMES','CLERK',7698,'1981-12-03',950,NULL,30);\n",
    "INSERT INTO emp VALUES(7902,'FORD','ANALYST',7566,'1981-12-03',3000,NULL,20);\n",
    "INSERT INTO emp VALUES(7934,'MILLER','CLERK',7782,'1983-01-23',1300,NULL,10);\n",
    "INSERT INTO emp VALUES(8888,'HIVE','PROGRAM',7839,'1988-01-23',10300,NULL);\n",
    "\n",
    "\n",
    "--创建dept表\n",
    "\n",
    "create table dept\n",
    "(\n",
    "deptno int  ,\n",
    "dname string,\n",
    "location string\n",
    ")ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n",
    "\n",
    "\n",
    "\n",
    "INSERT INTO dept VALUES(10,'ACCOUNTING','NEW YORK');\n",
    "INSERT INTO dept VALUES(20,'RESEARCH','DALLAS');\n",
    "INSERT INTO dept VALUES(30,'SALES','CHICAGO');\n",
    "INSERT INTO dept VALUES(40,'OPERATIONS','BOSTON');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### [第2章 Spark及其生态圈概述](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "https://spark.apache.org/\n",
    "### 2.2. Spark 概述及特点\n",
    "分布式计算框架\n",
    "相比hadoop速度快很多\n",
    " <img src='https://spark.apache.org/images/logistic-regression.png' >\n",
    "### 2.2. Spark 产生背景\n",
    "* mapreduce的局限性\n",
    "  * 代码繁琐\n",
    "  * 只能支持map和reduce操作\n",
    "  * 执行效率低下\n",
    "  * 不适合迭代多次、交互式、流式的处理\n",
    "* 框架多样化\n",
    "  * 批处理（离线）：MapReduce，Hive，Pig\n",
    "  * 流式处理（实时）：storm，jstom\n",
    "  * 交互式计算：Impala   \n",
    "  \n",
    "> 框架多样化，导致需要多态集群，学习运维成本高\n",
    "\n",
    "### 2.4. Spark发展历史\n",
    "<img src='img/Spark发展历史.png'，width='66%'>\n",
    "### 2.6. Spark 对比 Hadoop\n",
    "### 2.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# [第3章 实战环境搭建](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Spark 源码编译\n",
    "* 下载  wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0.tgz\n",
    "* 解压  解压：tar -zxvf spark-2.1.0.tgz -C ~/app/\n",
    "* 要求： Building Spark using Maven requires Maven 3.3.9 or newer and Java 7+.\n",
    "* Maven3.3.9安装包下载 wget https://mirrors.tuna.tsinghua.edu.cn/apache//maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz\n",
    "* 解压： tar -zxvf apache-maven-3.3.9-bin.tar.gz -C ~/app/\n",
    "* maven内存设置 export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"\n",
    "* 编译命令\n",
    "  * ./build/mvn -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver  -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package\n",
    "\n",
    "  * 推荐： \n",
    "  \n",
    "  ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0  --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0  -Pmesos \n",
    "  \n",
    "  ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0  --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0  -Pmesos \n",
    "  \n",
    "    第二个编译完成后：spark-\\$VERSINO-bin-\\$NAME \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scala 安装配置哦\n",
    "wgethttps://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz\n",
    "解压：tar -zxvf scala-2.11.8.tgz -C ~/app/\n",
    "设置Scala环境变量设置\n",
    "\n",
    "   \n",
    "验证Scala安装  \n",
    "\n",
    "scala -version  \n",
    "Scala code runner version 2.11.7 -- Copyright 2002-2013, LAMP/EPFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-63b03db53f5c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-63b03db53f5c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    (/将JDK目录添加到系统环境变量(~/.bash_profile)中)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " //将JDK目录添加到系统环境变量(~/.bash_profile)中 \n",
    "export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9 \n",
    "export PATH=$MAVEN_HOME/bin:$PATH \n",
    "//让配置文件生效 source ~/.bash_profile\n",
    "//执行mvn，查看版本\n",
    "mvn -v \n",
    "//如果安装成功后，则有如下信息的输出 \n",
    "Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T08:41:47-08:00) \n",
    "Maven home: /home/hadoop/app/apache-maven-3.3.9 Java version: 1.7.0_51, vendor: Oracle Corporation \n",
    "Java home: /home/hadoop/app/jdk1.7.0_51/jre \n",
    "Default locale: zh_CN, platform encoding: UTF-8 \n",
    "OS name: \"linux\", version: \"2.6.32-358.el6.x86_64\", arch: \"amd64\", family: \"unix\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坑1：编译报错；\n",
    "Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.1.0: Failure to find org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 in https://repo1.maven.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]\n",
    "\n",
    "坑2：虚拟机内存：2-4G\n",
    "坑3： ./dev/change-scala-version.sh 2.11\n",
    "\n",
    "坑4：zinc https://blog.csdn.net/jiaotangX/article/details/78635133\n",
    "\n",
    "https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html  \n",
    "https://spark.apache.org/docs/2.1.0/building-spark.html  \n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571\n",
    "\n",
    "坑：zinc https://blog.csdn.net/jiaotangX/article/details/78635133  \n",
    "部分为修改项（此处是导致编译失败的根本原因）\n",
    "```\n",
    "<scalaVersion>${scala.version}</scalaVersion>\n",
    "<recompileMode>incremental</recompileMode>\n",
    "<useZincServer>false</useZincServer>\n",
    "\n",
    "```\n",
    "\n",
    "坑：\n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571\n",
    "http://www.ijava.com/article/hadoop-42617.html\n",
    "https://blog.csdn.net/wuzhilon88/article/details/73240571\n",
    "从失败的地方开始： \n",
    "https://blog.csdn.net/suisongtiao1799/article/details/80223068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e2d5d8f90c5f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e2d5d8f90c5f>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    <repository>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 报错，在pom.xml的<id>central</id>下插入\n",
    "\n",
    " \n",
    "<repository>\n",
    "    <id>cloudera</id>\n",
    "       <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n",
    "</repository>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<properties>\n",
    " \n",
    "    <hadoop.version>2.2.0</hadoop.version>\n",
    "    <protobuf.version>2.5.0</protobuf.version>\n",
    "    <yarn.version>${hadoop.version}</yarn.version>\n",
    "</properties>\n",
    "\n",
    "\n",
    "<profile>\n",
    "  <id>hadoop-2.6</id>\n",
    "  <properties>\n",
    "    <hadoop.version>2.6.4</hadoop.version>\n",
    "    <jets3t.version>0.9.3</jets3t.version>\n",
    "    <zookeeper.version>3.4.6</zookeeper.version>\n",
    "    <curator.version>2.6.0</curator.version>\n",
    "  </properties>\n",
    "</profile>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Spark Local模式环境搭建\n",
    "\n",
    "* 解压： tar -zxvf spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz -C ~/app/\n",
    "\n",
    "* 删除cmd： rm -rf *.cmd \n",
    "\n",
    "```\n",
    "export SPARK_HOME=/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0 \n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "```\n",
    " `spark-shell --master local[2]`\n",
    "\n",
    "\n",
    "### 3.5. Spark Standalone模式环境搭建\n",
    "与HDFS/YARN很类似\n",
    "1.master + n worker\n",
    "\n",
    "\n",
    "[hadoop@master conf]$ `cp spark-env.sh.template spark-env.sh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e0a65de057de>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e0a65de057de>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    ``` spark-env.sh\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Options for the daemons used in the standalone deploy mode\n",
    "\n",
    "``` spark-env.sh\n",
    "PARK_MASTER_HOST=master   \n",
    "SPARK_WORKER_CORES=2\n",
    "SPARK_WORKER_MEMORY=2g\n",
    "SPARK_WORKER_INSTANCES=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "http://www.voidcn.com/article/p-opptahtj-pb.html\n",
    "\n",
    " `SPARK_HOME/sbin/start-all.sh`\n",
    " \n",
    "\n",
    "* 提示：JAVA_HOME is not set的错误提示。\n",
    "https://blog.csdn.net/qq_27882063/article/details/58599276\n",
    "在spark-env.sh追加  JAVA_HOME 相关参数\n",
    "* 2:sbin目录一般与服务相关\n",
    " \n",
    "如果：  \n",
    "hadoop1:master  \n",
    "hadoop2:worker  \n",
    "hadoop2:worker  \n",
    "然后配置：slaves.sh  \n",
    "hadoop2  \n",
    "hadoop3  \n",
    "===》start-all.sh\n",
    "会在master上启动master进程，在slaves文件配置的所以hostname的机器上启动worker进程\n",
    "**启动**\n",
    "`spark-shell --master spark://master:7077 `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.spark 简单使用wordcount\n",
    "wordcounts\n",
    "```scala\n",
    "val file=spark.sparkContext.textFile(\"file:///home/hadoop/data/wc.txt\")\n",
    "val wordCounts = file.flatMap(line => line.split(\" ,\")).map((word => (word,1))).reduceByKey(_ + _)\n",
    "wordCounts.collect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =4></a>\n",
    "# [第4章 Spark SQL概述](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 前世今生\n",
    "关系型数据库  RDD DML\n",
    "\n",
    "* Hive: 类似于sql的Hive QL语言，sql====>mapreduce\n",
    "* Spark：hive on spark   ＝＝＝＝＞　ｓｈａｒｋ基于spark、基于内存的列式存储、与hive兼容。缺点：hive ql解析、逻辑执行。。。。。。依赖hive\n",
    "* shark终止后： 1，hive on spark 2,Spark SQL\n",
    "### 4.2. SQL on hadoop 框架\n",
    "* Hive metastone:元数据 slq to mepreduce\n",
    "* impala : cloudera开发（cdh，cm） sql\n",
    "* presto: facebook开源（京东）sql\n",
    "\n",
    "* drill:  sql 访问：hadf,rdbms json habse mangodb s3 hive\n",
    "\n",
    "* Spark Sql: sql dataframe/dataset api\n",
    "   * matastroen \n",
    "   * 访问：hadf,rdbms json habse mangodb s3 hive ===> 数据源\n",
    "   \n",
    "<img src='img/Spark_SQL架构.png' width='77%'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 5></a>\n",
    "# [第5章 从Hive平滑过渡到Spark SQL](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SQLContext 的使用\n",
    "> Spark 1.x中Spark　SQL的入口\n",
    "\n",
    "\n",
    "```Scala\n",
    "val sc: SparkContext // An existing SparkContext.\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "```\n",
    "\n",
    "### 5.2 HiveContext\n",
    "\n",
    "### 5.4 SparkSession\n",
    "> Spark 1.x中Spark　SQL的入口\n",
    "\n",
    "```Scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._\n",
    "\n",
    "```\n",
    "\n",
    "win 路径写法:`file:///D:/Data/people.json`\n",
    "\n",
    "打jar包到客户端: `D:\\Documents\\Spark\\SparkSQL>mvn clean package -DskipTests`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching Applications with spark-submit\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "    \n",
    "# examoles\n",
    "# 提交\n",
    "./bin/spark-submit \\\n",
    " --class com.imooc.spark.SQLContextAPP \\\n",
    " --master local[2] \\\n",
    " /home/hadoop/lib/sql-1.0.jar \\\n",
    "/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 spark-shell 和spark-sql 的使用\n",
    " `spark-shell --master local[2]`\n",
    " \n",
    " * spark 控制台:   \n",
    " `spark.sql(\"show tables\").show`  \n",
    " \n",
    "* 在 /home/hadoop/app/hive-1.1.0-cdh5.7.0/conf目录下:  \n",
    " `cp hive-site.xml ~/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/conf/`\n",
    " \n",
    "* 传入 jars包  \n",
    "  `spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.46-bin.jar`\n",
    "\n",
    "之后可以查看数据库的数据\n",
    "\n",
    "* 应用:\n",
    "\n",
    "hive>\n",
    "` select * from emp e join dept d on e.deptno=d.deptno;`\n",
    "\n",
    "\n",
    "> Time taken: 43.799 seconds, Fetched: 14 row(s)\n",
    "\n",
    "\n",
    "scala>\n",
    "`spark.sql(\"select * from emp e join dept d on e.deptno=d.deptno\").show`\n",
    "> 立即出结果\n",
    "\n",
    "\n",
    "* 处理报错ERROR\n",
    "修改:`/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/conf/hive-site.xml `\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
